{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 각종 설정\n",
    "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream_model_file: epoch=2-val_loss=0.287-val_acc=0.897.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": "              key                                                      value\n0        hostname                                                      dl012\n1        hostaddr                                             129.254.182.78\n2     python_path  /data/dlt/mambaforge/envs/DeepKorean-23.03/bin/python3.10\n3    project_name                                                 DeepKorean\n4    project_path                            /data/dlt/proj/DeepKorean-23.03\n5    working_path                            /data/dlt/proj/DeepKorean-23.03\n6    running_file                                tests/1-doc_cls-infer.ipynb\n7    working_gpus                                                          0\n8  number_of_gpus                                                          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hostname</td>\n      <td>dl012</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hostaddr</td>\n      <td>129.254.182.78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>python_path</td>\n      <td>/data/dlt/mambaforge/envs/DeepKorean-23.03/bin/python3.10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>project_name</td>\n      <td>DeepKorean</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>project_path</td>\n      <td>/data/dlt/proj/DeepKorean-23.03</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>working_path</td>\n      <td>/data/dlt/proj/DeepKorean-23.03</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>running_file</td>\n      <td>tests/1-doc_cls-infer.ipynb</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>working_gpus</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>number_of_gpus</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chrisbase.util import to_dataframe\n",
    "from chrislab.common.util import GpuProjectEnv\n",
    "from ratsnlp.nlpbook.classification import ClassificationDeployArguments\n",
    "\n",
    "with GpuProjectEnv(project_name=\"DeepKorean\", working_gpus=\"0\") as env:\n",
    "    args = ClassificationDeployArguments(\n",
    "        working_config_file=env.running_file.with_suffix('.json').name,\n",
    "        pretrained_model_path=\"model/pretrained/KcBERT-Base\",\n",
    "        downstream_model_path=\"model/finetuned/nsmc (dl012) [03.20 21:34:29]\",\n",
    "        downstream_model_file=None,\n",
    "        max_seq_length=128,\n",
    "    )\n",
    "    config = args.save_working_config()\n",
    "    assert config.exists(), f\"No config file: {config}\"\n",
    "to_dataframe(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 웹서비스 만들기 준비\n",
    "\n",
    "`ngrok`은 로컬에서 실행 중인 웹서비스를 안전하게 외부에서 접근 가능하도록 해주는 도구입니다.\n",
    "1. Signup: [https://dashboard.ngrok.com/signup](https://dashboard.ngrok.com/signup)\n",
    "2. Login : [https://dashboard.ngrok.com/login](https://dashboard.ngrok.com/login)\n",
    "3. Check : [https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
    "4. Config: ngrok config add-authtoken {authtoken}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from flask_ngrok import _download_ngrok\n",
    "\n",
    "_download_ngrok(env.working_path / \"ngrok\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 dlt etri 30137501 Mar 21 00:48 /data/dlt/proj/DeepKorean-23.03/ngrok/ngrok\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 777 \"{env.working_path}/ngrok/ngrok\"\n",
    "!ls -al \"{env.working_path}/ngrok/ngrok\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /data/dlt/.ngrok2/ngrok.yml\r\n"
     ]
    }
   ],
   "source": [
    "your_authtoken = \"2NHZJsBLbOgcBLEmZTmuvJaOJM2_2VHnFpENJjTg5dwsSLjiE\"  #https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "!\"{env.working_path}/ngrok/ngrok\" authtoken {your_authtoken}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authtoken: 2NHZJsBLbOgcBLEmZTmuvJaOJM2_2VHnFpENJjTg5dwsSLjiE\r\n",
      "version: \"2\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat \"$HOME/.ngrok2/ngrok.yml\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     key                                          value\n0    working_config_file                           1-doc_cls-infer.json\n1  pretrained_model_path                   model/pretrained/KcBERT-Base\n2  downstream_model_path  model/finetuned/nsmc (dl012) [03.20 21:34:29]\n3  downstream_model_file      epoch=2-val_loss=0.287-val_acc=0.897.ckpt\n4         max_seq_length                                            128",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>working_config_file</td>\n      <td>1-doc_cls-infer.json</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pretrained_model_path</td>\n      <td>model/pretrained/KcBERT-Base</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>downstream_model_path</td>\n      <td>model/finetuned/nsmc (dl012) [03.20 21:34:29]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>downstream_model_file</td>\n      <td>epoch=2-val_loss=0.287-val_acc=0.897.ckpt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>max_seq_length</td>\n      <td>128</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "config = Path(config)\n",
    "args = ClassificationDeployArguments.from_json(config.read_text())\n",
    "to_dataframe(args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 로딩\n",
    "파인튜닝을 마친 모델과 토크나이저를 읽어 들입니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tuned_model_ckpt.keys()=dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n      (position_embeddings): Embedding(300, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "fine_tuned_model_ckpt = torch.load(\n",
    "    Path(args.downstream_model_path) / args.downstream_model_file,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"fine_tuned_model_ckpt.keys()={fine_tuned_model_ckpt.keys()}\")\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_path,\n",
    "    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",
    ")\n",
    "model = BertForSequenceClassification(pretrained_model_config)\n",
    "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizer(name_or_path='model/pretrained/KcBERT-Base', vocab_size=30000, model_max_length=300, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_path,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 인퍼런스 함수 선언\n",
    "인퍼런스 함수를 선언합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def inference_fn(sentence):\n",
    "    inputs = tokenizer(\n",
    "        [sentence],\n",
    "        max_length=args.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()})\n",
    "        prob = outputs.logits.softmax(dim=1)\n",
    "        positive_prob = round(prob[0][1].item(), 4)\n",
    "        negative_prob = round(prob[0][0].item(), 4)\n",
    "        pred = \"긍정 (positive)\" if torch.argmax(prob) == 1 else \"부정 (negative)\"\n",
    "    return {\n",
    "        'sentence': sentence,\n",
    "        'prediction': pred,\n",
    "        'positive_data': f\"긍정 {positive_prob}\",\n",
    "        'negative_data': f\"부정 {negative_prob}\",\n",
    "        'positive_width': f\"{positive_prob * 100}%\",\n",
    "        'negative_width': f\"{negative_prob * 100}%\",\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 웹서비스 개시\n",
    "아래처럼 실행해 인퍼런스 함수를 웹서비스로 만듭니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'ratsnlp.nlpbook.classification.deploy'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m\u001B[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001B[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001B[33mPress CTRL+C to quit\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://2141-129-254-182-78.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2023 00:57:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2023 00:57:20] \"\u001B[33mGET /favicon.ico HTTP/1.1\u001B[0m\" 404 -\n",
      "127.0.0.1 - - [21/Mar/2023 00:57:24] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2023 00:57:32] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.classification import get_web_service_app\n",
    "\n",
    "app = get_web_service_app(inference_fn)\n",
    "app.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
